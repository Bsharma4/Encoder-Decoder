{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://stackabuse.com/python-for-nlp-neural-machine-translation-with-seq2seq-in-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the train_source file in read mode\n",
    "my_file = open(\"train-source.txt\", \"r\", encoding='UTF-8')\n",
    "\n",
    "# reading the file\n",
    "data = my_file.read()\n",
    "\n",
    "train_source_list = data.replace('\\n',\" \").split('</s>')\n",
    "\n",
    "my_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the train-target file in read mode\n",
    "my_file = open(\"train-target.txt\", \"r\", encoding='UTF-8')\n",
    "\n",
    "# reading the file\n",
    "data = my_file.read()\n",
    "\n",
    "train_target_list = data.replace('\\n',\" \").split('</s>')\n",
    "\n",
    "my_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the test-target file in read mode\n",
    "my_file = open(\"test-source.txt\", \"r\", encoding='UTF-8')\n",
    "\n",
    "# reading the file\n",
    "data = my_file.read()\n",
    "\n",
    "test_source_list = data.replace('\\n',\" \").split('</s>')\n",
    "\n",
    "my_file.close()\n",
    "\n",
    "# opening the test-target file in read mode\n",
    "my_file = open(\"test-target.txt\", \"r\", encoding='UTF-8')\n",
    "\n",
    "# reading the file\n",
    "data = my_file.read()\n",
    "\n",
    "test_target_list = data.replace('\\n',\" \").split('</s>')\n",
    "\n",
    "my_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"length of training sets Source: \", len(train_source_list), \" Target :\", len(train_target_list))\n",
    "print(\"length of test sets Source: \", len(test_source_list), \" Target :\", len(test_target_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples input: 45172\n",
      "num samples output: 45172\n",
      "num samples output input: 45172\n"
     ]
    }
   ],
   "source": [
    "input_sentences = train_source_list\n",
    "output_sentences = train_target_list\n",
    "output_sentences_inputs = output_sentences\n",
    "\n",
    "\n",
    "print(\"num samples input:\", len(input_sentences))\n",
    "print(\"num samples output:\", len(output_sentences))\n",
    "print(\"num samples output input:\", len(output_sentences_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <s> Ní rabh aon duine fá'n áit a rabh fhios aige dadadh fá dtaobh de . \n",
      " <s> Ní raibh aon duine faoin áit a raibh a fhios aige dada fá dtaobh de . \n",
      " <s> Ní raibh aon duine faoin áit a raibh a fhios aige dada fá dtaobh de . \n"
     ]
    }
   ],
   "source": [
    "print(input_sentences[172])\n",
    "print(output_sentences[172])\n",
    "print(output_sentences_inputs[172])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_unique_words = set()\n",
    "for sentence in train_source_list:\n",
    "    input_unique_words.update(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of unique word in input:  107\n"
     ]
    }
   ],
   "source": [
    "print(\"no. of unique word in input: \",len(input_unique_words))\n",
    "MAX_NUM_WORDS=len(input_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the input: 27496\n",
      "Length of longest sentence in input: 104\n"
     ]
    }
   ],
   "source": [
    "input_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "input_tokenizer.fit_on_texts(input_sentences)\n",
    "input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)\n",
    "\n",
    "word2idx_inputs = input_tokenizer.word_index\n",
    "print('Total unique words in the input: %s' % len(word2idx_inputs))\n",
    "\n",
    "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
    "print(\"Length of longest sentence in input: %g\" % max_input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the output: 25738\n",
      "Length of longest sentence in the output: 103\n"
     ]
    }
   ],
   "source": [
    "output_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
    "output_tokenizer.fit_on_texts(output_sentences + output_sentences_inputs)\n",
    "output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n",
    "output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n",
    "\n",
    "word2idx_outputs = output_tokenizer.word_index\n",
    "print('Total unique words in the output: %s' % len(word2idx_outputs))\n",
    "\n",
    "num_words_output = len(word2idx_outputs) + 1\n",
    "max_out_len = max(len(sen) for sen in output_integer_seq)\n",
    "print(\"Length of longest sentence in the output: %g\" % max_out_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "EPOCHS = 2\n",
    "LSTM_NODES =232\n",
    "NUM_SENTENCES = 20000\n",
    "MAX_SENTENCE_LENGTH = max_out_len\n",
    "MAX_NUM_WORDS = len(input_unique_words)\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_sequences.shape: (45172, 104)\n",
      "encoder_input_sequences[172]: [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   1  14  19  90  62  89   2  19 104  61  52  29]\n"
     ]
    }
   ],
   "source": [
    "encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=max_input_len)\n",
    "print(\"encoder_input_sequences.shape:\", encoder_input_sequences.shape)\n",
    "print(\"encoder_input_sequences[172]:\", encoder_input_sequences[172])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_input_sequences.shape: (45172, 103)\n",
      "decoder_input_sequences[172]: [  1  19  18  64  56 100   3  18   3 104  60  75  29   2   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')\n",
    "print(\"decoder_input_sequences.shape:\", decoder_input_sequences.shape)\n",
    "print(\"decoder_input_sequences[172]:\", decoder_input_sequences[172])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44981\n",
      "44930\n"
     ]
    }
   ],
   "source": [
    "input_token_index = dict([(char, i) for i, char in enumerate(train_source_list)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(train_target_list)])\n",
    "\n",
    "print(len(input_token_index))\n",
    "print(len(target_token_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_file = input_token_index\n",
    "\n",
    "# for line in glove_file:\n",
    "#     records = line.split()\n",
    "#     word = records[0]\n",
    "#     vector_dimensions = asarray(records[1:], dtype='float64')\n",
    "#     embeddings_dictionary[word] = vector_dimensions\n",
    "# glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
    "embedding_matrix = zeros((num_words, EMBEDDING_SIZE))\n",
    "for word, index in word2idx_inputs.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'agus'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m embeddings_dictionary[\u001b[39m'\u001b[39;49m\u001b[39magus\u001b[39;49m\u001b[39m'\u001b[39;49m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'agus'"
     ]
    }
   ],
   "source": [
    "embeddings_dictionary['agus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(embedding_matrix[\u001b[39m'\u001b[39;49m\u001b[39magus\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix['agus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(num_words, EMBEDDING_SIZE, weights=[embedding_matrix], input_length=max_input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 446. GiB for an array with shape (45172, 103, 25739) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [47], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m decoder_targets_one_hot \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mzeros((\n\u001b[0;32m      2\u001b[0m         \u001b[39mlen\u001b[39;49m(input_sentences),\n\u001b[0;32m      3\u001b[0m         max_out_len,\n\u001b[0;32m      4\u001b[0m         num_words_output\n\u001b[0;32m      5\u001b[0m     ),\n\u001b[0;32m      6\u001b[0m     dtype\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mfloat32\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 446. GiB for an array with shape (45172, 103, 25739) and data type float32"
     ]
    }
   ],
   "source": [
    "decoder_targets_one_hot = np.zeros((\n",
    "        len(input_sentences),\n",
    "        max_out_len,\n",
    "        num_words_output\n",
    "    ),\n",
    "    dtype='float32'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e5c4b6e65ccaabe18f9cd3218992e678d372855fbc859c2eb66bba448f3faf11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
