{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yW6wIhExqIhe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "R-vNkTagqNPs"
      },
      "outputs": [],
      "source": [
        "# opening the train_source file in read mode\n",
        "my_file = open(\"train-source.txt\", \"r\", encoding='UTF-8')\n",
        "\n",
        "# reading the file\n",
        "data = my_file.read()\n",
        "\n",
        "train_source_list = data.replace('\\n',\" \").split('</s>')\n",
        "\n",
        "my_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_source_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cMeA_KpUqgLk"
      },
      "outputs": [],
      "source": [
        "# opening the train-target file in read mode\n",
        "my_file = open(\"train-target.txt\", \"r\", encoding='UTF-8')\n",
        "\n",
        "# reading the file\n",
        "data = my_file.read()\n",
        "\n",
        "train_target_list = data.replace('\\n',\" \").split('</s>')\n",
        "\n",
        "my_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# opening the test-target file in read mode\n",
        "my_file = open(\"test-source.txt\", \"r\", encoding='UTF-8')\n",
        "\n",
        "# reading the file\n",
        "data = my_file.read()\n",
        "\n",
        "test_source_list = data.replace('\\n',\" \").split('</s>')\n",
        "\n",
        "my_file.close()\n",
        "\n",
        "# opening the test-target file in read mode\n",
        "my_file = open(\"test-target.txt\", \"r\", encoding='UTF-8')\n",
        "\n",
        "# reading the file\n",
        "data = my_file.read()\n",
        "\n",
        "test_target_list = data.replace('\\n',\" \").split('</s>')\n",
        "\n",
        "my_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmUu_iTXqmx1",
        "outputId": "976a419a-0cff-4f41-e86c-a6e7fc1bf3af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of training sets Source:  45172  Target : 45172\n",
            "length of test sets Source:  1001  Target : 1001\n"
          ]
        }
      ],
      "source": [
        "print(\"length of training sets Source: \", len(train_source_list), \" Target :\", len(train_target_list))\n",
        "print(\"length of test sets Source: \", len(test_source_list), \" Target :\", len(test_target_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finding unique words in both sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "44981\n",
            "44930\n"
          ]
        }
      ],
      "source": [
        "input_token_index = dict([(char, i) for i, char in enumerate(train_source_list)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(train_target_list)])\n",
        "\n",
        "print(len(input_token_index))\n",
        "print(len(target_token_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_unique_words = set()\n",
        "for sentence in train_source_list:\n",
        "    input_unique_words.update(sentence)\n",
        "\n",
        "output_unique_words = set()\n",
        "for sentence in train_target_list:\n",
        "    output_unique_words.update(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "107\n",
            "95\n"
          ]
        }
      ],
      "source": [
        "print(len(input_unique_words))\n",
        "print(len(output_unique_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "lang_eng = []\n",
        "lang_ita = []\n",
        "\n",
        "raw_data_en, raw_data_ita = train_source_list, train_target_list\n",
        "\n",
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer\n",
        "\n",
        "input_tensor, inp_lang = tokenize(train_source_list)\n",
        "target_tensor, targ_lang = tokenize(train_target_list)\n",
        "\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "36137 36137 9035 9035\n",
            "Input Language; index to word mapping\n",
            "1 ----> <s>\n",
            "8 ----> \"\n",
            "29 ----> tá\n",
            "59 ----> tú\n",
            "15 ----> i\n",
            "683 ----> n-am\n",
            "12 ----> go\n",
            "367 ----> leor\n",
            "5 ----> ,\n",
            "8 ----> \"\n",
            "20 ----> arsa\n",
            "296 ----> nóra\n",
            "14 ----> '\n",
            "288 ----> á\n",
            "6246 ----> comóradh\n",
            "14 ----> '\n",
            "155 ----> un\n",
            "4 ----> an\n",
            "541 ----> dorais\n",
            "2 ----> .\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <s>\n",
            "40 ----> `\n",
            "33 ----> tá\n",
            "55 ----> tú\n",
            "66 ----> in\n",
            "127 ----> am\n",
            "12 ----> go\n",
            "186 ----> leor\n",
            "5 ----> ,\n",
            "14 ----> '\n",
            "22 ----> arsa\n",
            "283 ----> nóra\n",
            "5 ----> ,\n",
            "128 ----> á\n",
            "4224 ----> comóradh\n",
            "62 ----> chun\n",
            "4 ----> an\n",
            "542 ----> dorais\n",
            "2 ----> .\n"
          ]
        }
      ],
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))\n",
        "\n",
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "\n",
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(1000, 232), dtype=tf.int32, name=None), TensorSpec(shape=(1000, 222), dtype=tf.int32, name=None))>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 1000\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, inp_vocab_size, embedding_size, lstm_size, input_length):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        #Initialize Embedding layer\n",
        "        #Intialize Encoder LSTM layer\n",
        "        \n",
        "        self.lstm_size = lstm_size\n",
        "        self.embedding = tf.keras.layers.Embedding(inp_vocab_size, embedding_size)\n",
        "        self.lstm = tf.keras.layers.LSTM(lstm_size, return_sequences=True, return_state=True)\n",
        "\n",
        "    def call(self, input_sequence, states):\n",
        "      \n",
        "        embed = self.embedding(input_sequence)\n",
        "        output, state_h, state_c = self.lstm(embed, initial_state=states)\n",
        "\n",
        "        return output, state_h, state_c\n",
        "    \n",
        "    def initialize_states(self,batch_size):\n",
        "    \n",
        "        return (tf.zeros([batch_size, self.lstm_size]),\n",
        "                tf.zeros([batch_size, self.lstm_size]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self,scoring_function, att_units):\n",
        "        super(Attention, self).__init__()\n",
        "        \n",
        "        self.scoring_function = scoring_function\n",
        "        self.att_units = att_units\n",
        "\n",
        "        if self.scoring_function=='dot':\n",
        "            pass\n",
        "            # For general, it would be self.wa = tf.keras.layers.Dense(att_units)\n",
        "\n",
        "\n",
        "    def call(self,decoder_hidden_state,encoder_output):\n",
        "\n",
        "        if self.scoring_function == 'dot':\n",
        "            \n",
        "            new_state = tf.expand_dims(decoder_hidden_state, -1)\n",
        "            score = tf.matmul(encoder_output, new_state)\n",
        "            weights = tf.nn.softmax(score, axis=1)\n",
        "            context = weights * encoder_output\n",
        "            context_vector = tf.reduce_sum(context, axis=1)\n",
        "                                \n",
        "            return context_vector, weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class One_Step_Decoder(tf.keras.Model):\n",
        "    def __init__(self, tar_vocab_size, embedding_dim, input_length, dec_units, score_fun, att_units):\n",
        "        super(One_Step_Decoder, self).__init__()\n",
        "        # Initialize decoder embedding layer, LSTM and any other objects needed\n",
        "        self.tar_vocab_size = tar_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.input_length = input_length\n",
        "        self.dec_units = dec_units\n",
        "        self.score_fun = score_fun\n",
        "        self.att_units = att_units\n",
        "        self.embedding = tf.keras.layers.Embedding(self.tar_vocab_size, self.embedding_dim, \n",
        "                                                   input_length=self.input_length)\n",
        "        \n",
        "        self.lstm = tf.keras.layers.LSTM(self.dec_units, return_sequences=True, \n",
        "                                         return_state=True)\n",
        "        \n",
        "        self.output_layer = tf.keras.layers.Dense(self.tar_vocab_size)\n",
        "        \n",
        "        self.attention = Attention(self.score_fun, self.att_units)\n",
        "\n",
        "    def call(self, input_to_decoder, encoder_output, state_h, state_c):\n",
        "        \n",
        "        result = self.embedding(input_to_decoder)\n",
        "        \n",
        "        context_vector, weights = self.attention(state_h, encoder_output)\n",
        "        \n",
        "        concat = tf.concat([tf.expand_dims(context_vector, 1), result], axis=-1)\n",
        "        \n",
        "        decoder_output, hidden_state, cell_state = self.lstm(concat, initial_state=[state_h, state_c])\n",
        "        \n",
        "        final_output = tf.reshape(decoder_output, (-1, decoder_output.shape[2]))\n",
        "        final_output = self.output_layer(final_output)\n",
        "        \n",
        "        return final_output, hidden_state, cell_state, weights, context_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, out_vocab_size, embedding_dim, output_length, dec_units ,score_fun ,att_units):\n",
        "        #Intialize necessary variables and create an object from the class onestepdecoder\n",
        "        super(Decoder, self).__init__()\n",
        "        self.out_vocab_size = out_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.output_length = output_length\n",
        "        self.dec_units = dec_units\n",
        "        self.score_fun = score_fun\n",
        "        self.att_units = att_units\n",
        "        self.onestepdecoder = One_Step_Decoder(self.out_vocab_size, self.embedding_dim, self.output_length,\n",
        "                                               self.dec_units, self.score_fun, self.att_units)\n",
        "        \n",
        "    def call(self, input_to_decoder,encoder_output,decoder_hidden_state,decoder_cell_state):\n",
        "        \n",
        "        all_outputs= tf.TensorArray(tf.float32, size=input_to_decoder.shape[1], name=\"output_arrays\")\n",
        "        \n",
        "        \n",
        "        for timestep in range(input_to_decoder.shape[1]):\n",
        "            output, decoder_hidden_state, decoder_cell_state, weights, context_vector = self.onestepdecoder(\n",
        "                                                                                    input_to_decoder[:,timestep:timestep+1], \n",
        "                                                                                    encoder_output, \n",
        "                                                                                    decoder_hidden_state,\n",
        "                                                                                    decoder_cell_state)\n",
        "            \n",
        "            all_outputs = all_outputs.write(timestep, output)\n",
        "        \n",
        "        all_outputs = tf.transpose(all_outputs.stack(), (1, 0, 2)) \n",
        "\n",
        "        return all_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "class encoder_decoder(tf.keras.Model):\n",
        "    def __init__(self, inp_vocab_size, out_vocab_size, embedding_size, lstm_size, \n",
        "                 input_length, output_length, dec_units ,score_fun ,att_units, batch_size):\n",
        "        \n",
        "        super(encoder_decoder, self).__init__()\n",
        "        \n",
        "        self.encoder = Encoder(inp_vocab_size, embedding_size, lstm_size, input_length)\n",
        "        self.decoder = Decoder(out_vocab_size, embedding_size, output_length, \n",
        "                               dec_units, score_fun, att_units)\n",
        "    \n",
        "    def call(self, data):\n",
        "        \n",
        "        input_sequence, input_to_decoder = data[0],data[1]\n",
        "        initial_state = self.encoder.initialize_states(batch_size=1000)\n",
        "        encoder_output, state_h, state_c = self.encoder(input_sequence, initial_state)\n",
        "        decoder_hidden_state = state_h\n",
        "        decoder_cell_state = state_c\n",
        "        decoder_output = self.decoder(input_to_decoder, encoder_output, decoder_hidden_state, decoder_cell_state)\n",
        "        \n",
        "        return decoder_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"dot.h5\", monitor='val_loss', verbose=1, save_weights_only=True)\n",
        "\n",
        "logdir='logs'\n",
        "tensorboard_Visualization = TensorBoard(log_dir=logdir)\n",
        "\n",
        "input_vocab_size = len(inp_lang.word_index)+1\n",
        "output_vocab_size = len(targ_lang.word_index)+1\n",
        "\n",
        "input_len = max_length_inp\n",
        "output_len = max_length_targ\n",
        "\n",
        "lstm_size = 200\n",
        "att_units = 300\n",
        "dec_units = 200\n",
        "embedding_size = 300\n",
        "embedding_dim = 300\n",
        "score_fun = 'dot'\n",
        "steps = len(input_tensor)//1000\n",
        "batch_size=64\n",
        "\n",
        "model = encoder_decoder(input_vocab_size,output_vocab_size,embedding_size,lstm_size,input_len,output_len,dec_units,score_fun,att_units, batch_size)\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=model.layers[0],\n",
        "                                 decoder=model.layers[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden,enc_state = model.layers[0](inp, enc_hidden)\n",
        "\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<s>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      predictions = model.layers[1](dec_input,enc_output,enc_hidden,enc_state)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = model.layers[0].trainable_variables + model.layers[1].trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mCanceled future for execute_request message before replies were done"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "EPOCHS = 3\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = model.layers[0].initialize_states(1000)\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 1000 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "      \n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_sentence(s):\n",
        "    s = re.sub(r'([!.?])', r' \\1', s)\n",
        "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "    s = re.sub(r'\\s+', r' ', s)\n",
        "\n",
        "    s = s.strip()\n",
        "    s = s +' '+' <end>'\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_sentence(input):\n",
        "    return('<s>' + input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(input_sentence):\n",
        "\n",
        "  attention_plot = np.zeros((output_len, input_len))\n",
        "\n",
        "  input_sentence = preprocess_sentence(input_sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in input_sentence]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=input_len,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "  \n",
        "  encoder_output,state_h,state_c = model.layers[0](inputs,[tf.zeros((1, lstm_size)),tf.zeros((1, lstm_size))])\n",
        "\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<s>']], 0)\n",
        "\n",
        "  for t in range(output_len):\n",
        "   predictions,state_h,state_c,attention_weights,context_vector = model.layers[1].onestepdecoder(dec_input,\n",
        "                                                                                                 encoder_output,\n",
        "                                                                                                 state_h,\n",
        "                                                                                                 state_c)\n",
        "\n",
        "   attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "   attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "   predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "   result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "   if targ_lang.index_word[predicted_id] == '<end>':\n",
        "     return result, input_sentence, attention_plot\n",
        "\n",
        "   dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, input_sentence, attention_plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "def translate(sentence):\n",
        "  result, sent, attention_plot = predict(sentence)\n",
        "\n",
        "  print('Input: %s' % (sent))\n",
        "  print('Predicted translation: {}'.format(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input sentnce:   <s> MÍ Iúil a bhí ann i mbliadhain a 1854 , nuair a bhain an taisme seo dúinn . \n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'i' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [59], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m test \u001b[39m=\u001b[39m test_source_list[\u001b[39m1\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minput sentnce: \u001b[39m\u001b[39m\"\u001b[39m, test)\n\u001b[1;32m----> 4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mpredicted sentence: \u001b[39m\u001b[39m\"\u001b[39m, translate(test))\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mActual target: \u001b[39m\u001b[39m\"\u001b[39m, test_target_list[\u001b[39m1\u001b[39m])\n",
            "Cell \u001b[1;32mIn [58], line 2\u001b[0m, in \u001b[0;36mtranslate\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtranslate\u001b[39m(sentence):\n\u001b[1;32m----> 2\u001b[0m   result, sent, attention_plot \u001b[39m=\u001b[39m predict(sentence)\n\u001b[0;32m      4\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mInput: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (sent))\n\u001b[0;32m      5\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mPredicted translation: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(result))\n",
            "Cell \u001b[1;32mIn [57], line 7\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(input_sentence)\u001b[0m\n\u001b[0;32m      3\u001b[0m attention_plot \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((output_len, input_len))\n\u001b[0;32m      5\u001b[0m input_sentence \u001b[39m=\u001b[39m preprocess_sentence(input_sentence)\n\u001b[1;32m----> 7\u001b[0m inputs \u001b[39m=\u001b[39m [inp_lang\u001b[39m.\u001b[39mword_index[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m input_sentence[i]]\n\u001b[0;32m      8\u001b[0m inputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mpreprocessing\u001b[39m.\u001b[39msequence\u001b[39m.\u001b[39mpad_sequences([inputs],\n\u001b[0;32m      9\u001b[0m                                                        maxlen\u001b[39m=\u001b[39minput_len,\n\u001b[0;32m     10\u001b[0m                                                        padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m inputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(inputs)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'i' is not defined"
          ]
        }
      ],
      "source": [
        "test = test_source_list[1]\n",
        "\n",
        "print(\"input sentnce: \", test)\n",
        "print(\"predicted sentence: \", translate(test))\n",
        "print(\"Actual target: \", test_target_list[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wma9nXxgDpwZ"
      },
      "outputs": [],
      "source": [
        "#Anything goes ref: https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
        "from nltk.translate.bleu_score import sentence_bleu,corpus_bleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(train_target_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Ní dheachaigh ceathrú uaire thart go bhfuair an ghirseach bheag bás .  \n"
          ]
        }
      ],
      "source": [
        "print(random.choice(train_target_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WxSdOvaWEqD",
        "outputId": "38468216-4d3f-4451-a979-77bce2af9317"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cumulative 1-gram: 1.000000\n",
            "Cumulative 2-gram: 1.000000\n",
            "Cumulative 3-gram: 1.000000\n",
            "Cumulative 4-gram: 1.000000\n"
          ]
        }
      ],
      "source": [
        "#For Train\n",
        "reference_train = train_source_list\n",
        "candidate_train = train_target_list[1]\n",
        "\n",
        "print('Cumulative 1-gram: %f' % sentence_bleu(train_source_list, train_target_list[1], weights=(1, 0, 0, 0)))\n",
        "print('Cumulative 2-gram: %f' % sentence_bleu(train_source_list, train_target_list[1], weights=(0.5, 0.5, 0, 0)))\n",
        "print('Cumulative 3-gram: %f' % sentence_bleu(train_source_list, train_target_list[1], weights=(0.33, 0.33, 0.33, 0)))\n",
        "print('Cumulative 4-gram: %f' % sentence_bleu(train_source_list, train_target_list[1], weights=(0.25, 0.25, 0.25, 0.25)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6sGz7XVyd8cg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cumulative 1-gram: 1.000000\n",
            "Cumulative 2-gram: 1.000000\n",
            "Cumulative 3-gram: 0.989740\n",
            "Cumulative 4-gram: 0.963897\n"
          ]
        }
      ],
      "source": [
        "#For Test\n",
        "reference_test = test_source_list\n",
        "candidate_test = test_target_list[1]\n",
        "\n",
        "print('Cumulative 1-gram: %f' % sentence_bleu(reference_test, candidate_test, weights=(1, 0, 0, 0)))\n",
        "print('Cumulative 2-gram: %f' % sentence_bleu(reference_test, candidate_test, weights=(0.5, 0.5, 0, 0)))\n",
        "print('Cumulative 3-gram: %f' % sentence_bleu(reference_test, candidate_test, weights=(0.33, 0.33, 0.33, 0)))\n",
        "print('Cumulative 4-gram: %f' % sentence_bleu(reference_test, candidate_test, weights=(0.25, 0.25, 0.25, 0.25)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "TpzMu-0jTNDC"
      },
      "outputs": [],
      "source": [
        "#From scratch: https://stackoverflow.com/questions/56968434/bleu-score-in-python-from-scratch\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "49CvGtpJXTs8"
      },
      "outputs": [],
      "source": [
        "def n_gram_generator(sentence,n= 2,n_gram= False):\n",
        "    '''\n",
        "    N-Gram generator with parameters sentence\n",
        "    n is for number of n_grams\n",
        "    The n_gram parameter removes repeating n_grams\n",
        "    '''\n",
        "    sentence = sentence.lower()  # converting to lower case\n",
        "    sent_arr = np.array(sentence.split())  # split to string arrays\n",
        "    length = len(sent_arr)\n",
        "\n",
        "    word_list = []\n",
        "    for i in range(length+1):\n",
        "        if i < n:\n",
        "            continue\n",
        "        word_range = list(range(i-n,i))\n",
        "        s_list = sent_arr[word_range]\n",
        "        string = ' '.join(s_list)  # converting list to strings\n",
        "        word_list.append(string) # append to word_list\n",
        "        if n_gram:\n",
        "            word_list = list(set(word_list))\n",
        "    return word_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "14VgL85DXZul"
      },
      "outputs": [],
      "source": [
        "def from_scratch_bleu_score(original, reference):\n",
        "    '''\n",
        "    Bleu score function given a orginal and a machine translated sentences\n",
        "    '''\n",
        "    rf_length = len(reference.split())\n",
        "    o_length  = len(original.split())\n",
        "\n",
        "    # Brevity Penalty\n",
        "    if rf_length > o_length:\n",
        "        BP=1\n",
        "    else:\n",
        "        penality=1-(rf_length/o_length)\n",
        "        BP = np.exp(penality)\n",
        "\n",
        "    # Clipped precision\n",
        "    clipped_precision_score = []\n",
        "    for ngram_level in range(1, 4):  # 1-gram to 4-gram\n",
        "        \n",
        "        \n",
        "        original_ngram_list = n_gram_generator(original, ngram_level)\n",
        "        original_n_gram = Counter(original_ngram_list)\n",
        "        \n",
        "        reference_ngram_list = n_gram_generator(reference, ngram_level)\n",
        "        reference_n_gram = Counter(reference_ngram_list)\n",
        "        \n",
        "        \n",
        "        num_ngrams_in_translation = sum(reference_n_gram.values())  # number of ngrams in translation\n",
        "        \n",
        "        # iterate the unique ngrams in translation (candidate)\n",
        "        for j in reference_n_gram:\n",
        "            \n",
        "            if j in original_n_gram:  # if found in reference\n",
        "                \n",
        "                if reference_n_gram[j] > original_n_gram[j]:  # CLIPPING - if found in translation more than in source, clip\n",
        "                    reference_n_gram[j] = original_n_gram[j]\n",
        "                    \n",
        "            else:\n",
        "                reference_n_gram[j] = 0\n",
        "\n",
        "        #print (sum(machine_n_gram.values()), c)\n",
        "        clipped_precision_score.append(float(sum(reference_n_gram.values())) / num_ngrams_in_translation)\n",
        "\n",
        "    #print (clipped_precision_score)\n",
        "\n",
        "    weights = [0.25]*4\n",
        "\n",
        "    s = (w_i * math.log(p_i) for w_i, p_i in zip(weights, clipped_precision_score))\n",
        "    s = BP * math.exp(math.fsum(s))\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKNPslQhTP6G",
        "outputId": "4c674081-c120-4cac-cf4f-d907e0c4d886"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU Score from scratch for Train files :  0.36787632499277756\n"
          ]
        }
      ],
      "source": [
        "#For Train Files\n",
        "original_train = train_source_list[1]\n",
        "reference_train = train_target_list[1]\n",
        "\n",
        "print (\"BLEU Score from scratch for Train files : \", from_scratch_bleu_score(original_train, reference_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU Score from scratch for Train files :  0.7539352394200599\n"
          ]
        }
      ],
      "source": [
        "candidate = random.randint(0,len(train_target_list))\n",
        "original_train = train_source_list[candidate]\n",
        "reference_train = train_target_list[candidate]\n",
        "From_Scratch_Score = from_scratch_bleu_score(original_train, reference_train)\n",
        "print (\"BLEU Score from scratch for Train files : \", From_Scratch_Score)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25376\n"
          ]
        }
      ],
      "source": [
        "print(random.randint(0,len(train_target_list)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "original = random.choice(train_source_list)\n",
        "print(original.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU Score from scratch for Test files :  0.47287080450158786\n"
          ]
        }
      ],
      "source": [
        "#For Train Files\n",
        "original_test = test_source_list[1]\n",
        "reference_test = test_target_list[1]\n",
        "\n",
        "print (\"BLEU Score from scratch for Test files : \", from_scratch_bleu_score(original_test, reference_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Approach - Seq to Seq encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "e5c4b6e65ccaabe18f9cd3218992e678d372855fbc859c2eb66bba448f3faf11"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
